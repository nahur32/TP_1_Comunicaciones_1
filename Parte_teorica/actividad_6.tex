\subsection*{6. Teoría de la Información y Capacidad de Canal}

\textbf{6.1) Conceptos Fundamentales:}
\begin{enumerate}[label=\alph*)]
    \item Defina información e información propia (auto-información) de un evento
    \item ¿Por qué se utiliza el logaritmo en la definición de información?
    \item Explique el concepto de entropía de una fuente discreta
\end{enumerate}

\textbf{6.2)} Para una fuente binaria con probabilidades \(P(0) = 0.3\) y \(P(1) = 0.7\):
\begin{enumerate}[label=\alph*)]
    \item Calcule la información propia de cada símbolo
    \item Calcule la entropía de la fuente
    \item Compare con la entropía máxima posible
\end{enumerate}

\textbf{6.3) Capacidad de Canal:}
\begin{enumerate}[label=\alph*)]
    \item Enuncie el Teorema de Shannon-Hartley
    \item ¿Qué representa físicamente la capacidad de canal?
    \item Para un canal AWGN con ancho de banda \(B = 4\) kHz y \(SNR = 15\) dB, calcule la capacidad del canal
\end{enumerate}